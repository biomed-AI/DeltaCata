{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504396ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from urllib import request\n",
    "from zeep import Client\n",
    "import hashlib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import ast\n",
    "import pubchempy as pcp\n",
    "from Bio import Entrez\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1b4ae",
   "metadata": {},
   "source": [
    "# 1. Download raw data from BRENDA\n",
    "To download data from BRENDA, a registration is needed (https://www.brenda-enzymes.org/register.php). The downloading process can take a couple of hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsdl = \"https://www.brenda-enzymes.org/soap/brenda_zeep.wsdl\"\n",
    "\n",
    "email = \"your email address\" # register in https://www.brenda-enzymes.org/\n",
    "brenda_password = 'your password'\n",
    "password = hashlib.sha256(brenda_password.encode(\"utf-8\")).hexdigest()\n",
    "client = Client(wsdl)\n",
    "parameters = (email, password)\n",
    "EC_numbers = client.service.getEcNumbersFromEcNumber(*parameters)\n",
    "print(\"There are %s different EC numbers in the BRENDA database.\" % len(EC_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56eac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data_save_path = \"./brenda_data_cache/EC_number_web_result/\"\n",
    "os.makedirs(web_data_save_path,exist_ok=True)\n",
    "for ec in EC_numbers:\n",
    "    url = f\"https://www.brenda-enzymes.org/enzyme.php?ecno={ec}#TURNOVER%20NUMBER%20[1/s]\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        with open(web_data_save_path+f\"{ec}.web\",'w') as f:\n",
    "            f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_kcat_km(target):\n",
    "    dataset = []\n",
    "    target_DivId = 'tab44' if target=='kcat' else 'tab12'\n",
    "    for EC_web_file_name in tqdm(os.listdir(web_data_save_path)):\n",
    "        file = open(web_data_save_path+EC_web_file_name,'r').read()\n",
    "\n",
    "        EC_number = EC_web_file_name.split('.web')[0]\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "        table_div = soup.find('div', id=target_DivId)\n",
    "        # Make sure to find the header\n",
    "        if table_div:\n",
    "            # Get the table row, the subsequent rows of the table will be obtained from id='tab44/tab12'\n",
    "            table_rows = table_div.find_all('div', recursive=False)\n",
    "            for row in table_rows:\n",
    "                cells = row.find_all('div', class_='cell')\n",
    "                # Extract the contents of each cell\n",
    "                if len(cells) == 7:  # Make sure the row has 7 columns of data\n",
    "                    kinetic = cells[0].text.strip()  # kcat or Km value\n",
    "                    substrate = cells[1].text.strip()  \n",
    "                    organism = cells[2].text.strip()  \n",
    "                    uniprot = cells[3].text.strip()  # UniProt ID\n",
    "                    commentary = cells[4].text.strip().lower()  \n",
    "                    literature = cells[5].text.strip()  # Literature ID\n",
    "                    if 'entries' in organism:\n",
    "                        continue\n",
    "                    row_data = [EC_number,organism, uniprot,literature,substrate, kinetic, commentary]\n",
    "                    dataset.append(row_data)\n",
    "\n",
    "        else:\n",
    "            print(EC_number)\n",
    "    return dataset\n",
    "\n",
    "kcat_dataset = process_kcat_km('kcat')\n",
    "km_dataset = process_kcat_km('km')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c508a1",
   "metadata": {},
   "source": [
    "# 2. Data cleaning\n",
    "Get SMILES and clean up missing entries or abnormal data. Downloading data from PubChem can take a couple of hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be440ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kcat_dataset))\n",
    "kcat_dataset = [row for row in kcat_dataset if row[5] != 'additional information']\n",
    "kcat_dataset = [row for row in kcat_dataset if 'entries' not in row[1]]\n",
    "kcat_dataset = [row for row in kcat_dataset if len(row[2].split(';'))==1] # rm multi UniProt IDs\n",
    "kcat_dataset = [row for row in kcat_dataset if len(row[2].split(','))==1] # rm multi UniProt IDs\n",
    "kcat_dataset = [row for row in kcat_dataset if ('-' not in row[5]) and (float(row[5])<100000)]\n",
    "kcat_dataset = [row for row in kcat_dataset if ('-' not in row[5]) and (float(row[5])>0.00001)]\n",
    "print(len(kcat_dataset))\n",
    "\n",
    "print(len(km_dataset))\n",
    "km_dataset = [row for row in km_dataset if row[5] != 'additional information']\n",
    "km_dataset = [row for row in km_dataset if 'entries' not in row[1]]\n",
    "km_dataset = [row for row in km_dataset if len(row[2].split(';'))==1]\n",
    "km_dataset = [row for row in km_dataset if len(row[2].split(','))==1]\n",
    "km_dataset = [row for row in km_dataset if ('-' not in row[5]) and (float(row[5])<100000)]\n",
    "km_dataset = [row for row in km_dataset if ('-' not in row[5]) and (float(row[5])>0.00001)]\n",
    "print(len(km_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete data if SMILES cannot be obtained based on the substrate or the SMILES format is abnormal.\n",
    "subs = list(set([row[4] for row in kcat_dataset]+[row[4] for row in km_dataset]))\n",
    "smiles_dict={}\n",
    "for sub in tqdm(subs):\n",
    "    comp=get_comp(sub)\n",
    "    if comp == -1: # retry if failed\n",
    "        comp=get_comp(sub)\n",
    "    if comp == -1:\n",
    "        smiles_dict[sub] = -1\n",
    "        continue\n",
    "    smiles = comp.canonical_smiles\n",
    "    smiles_dict[sub]=-1 if not smiles or '.' in smiles else comp\n",
    "pickle.dump(smiles_dict,open(\"./brenda_data_cache/smiles_dict.pkl\",'wb'))\n",
    "\n",
    "kcat_dataset = [row for row in kcat_dataset if smiles_dict[row[4]]!=-1]\n",
    "km_dataset = [row for row in km_dataset if smiles_dict[row[4]]!=-1]\n",
    "print(len(kcat_dataset))\n",
    "print(len(km_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean comments by normalizing stray Windows-1252 characters\n",
    "kcat_dataset_clean = sanitize_column_text(kcat_dataset)\n",
    "km_dataset_clean = sanitize_column_text(km_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a41358",
   "metadata": {},
   "source": [
    "# 3. Parse experimental conditions\n",
    "Extract pH, temperature, cosubstrate and buffer from the **comments** using regular expressions and LLM, then perform an initial clustering.\n",
    "\n",
    "Cluster records that share the same **EC number, species, UniProt ID, substrate, literature, temperature, pH, cosubstrate and buffer**. Then, discard clusters containing only wild-type or only mutant entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d4660",
   "metadata": {},
   "source": [
    "#### (a) Extract pH and temperature fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f01bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_dataset_with_ph_temp = Add_temperature_pH_fieds(kcat_dataset_clean)\n",
    "km_dataset_with_ph_temp = Add_temperature_pH_fieds(km_dataset_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea2590",
   "metadata": {},
   "source": [
    "#### (b) Initial clustering\n",
    "The initial clustering is intended to exclude unreasonable clusters, thus reducing the workload of subsequent LLM extraction and manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(dataset):\n",
    "    cluster_dict={}\n",
    "    for row in dataset:\n",
    "        pair_name = ';;;'.join([row[0],row[1],row[2],row[3],row[4],row[7],row[8]])\n",
    "        if pair_name not in cluster_dict:\n",
    "            cluster_dict[pair_name]=[]\n",
    "        cluster_dict[pair_name].append(row)\n",
    "    print(\"Same EC number, species, uniprotid, substrate, reference, temperature, pH, cluster: \",len(cluster_dict))\n",
    "\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        # Only keep rows that are wild-type or have at least one extracted mutation; drop everything else\n",
    "        new_rows = [] \n",
    "        for row in rows:\n",
    "            mutant_extracted = extract_mutations(row[6])\n",
    "            is_wildtype = 'wild' in row[6]\n",
    "            if (len(mutant_extracted)==0 and not is_wildtype) or (len(mutant_extracted)!=0 and is_wildtype):\n",
    "                continue\n",
    "            new_rows.append(row)\n",
    "        cluster_dict[pair_name]=new_rows\n",
    "        \n",
    "        # Extract the description fragment of cosubstrate and further subdivide the cluster\n",
    "        comments=[row[6] for row in new_rows]\n",
    "        cosub_extracted = [[i for i in c.split(',') if 'cosubstrate' in i or 'co-substrate' in i][0] if 'cosubstrate' in c or 'co-substrate' in c else -1 for c in comments]\n",
    "        if len(set(cosub_extracted))<=1: # All are the same cosub\n",
    "            continue\n",
    "        cluster_dict.pop(pair_name) \n",
    "        for cosub_info in set(cosub_extracted):\n",
    "            new_pair_name = pair_name+\";;;\"+str(cosub_info)\n",
    "            cluster_dict[new_pair_name]=[]\n",
    "        for cosub_info,row in zip(cosub_extracted,new_rows):\n",
    "            cluster_dict[pair_name+\";;;\"+str(cosub_info)].append(row)\n",
    "                        \n",
    "    # Delete clusters without wildtype or mutant\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        new_rows = cluster_dict[pair_name]\n",
    "        wildtype = [1 for row in new_rows if 'wild' in row[6]]\n",
    "        muttype = [1 for row in new_rows if len(extract_mutations(row[6]))>0]\n",
    "        if len(new_rows)<2 or sum(wildtype)==0 or sum(muttype)==0:\n",
    "            cluster_dict.pop(pair_name)\n",
    "    print(\"remain cluster: \",len(cluster_dict))\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "\n",
    "kcat_cluster_init = get_cluster(kcat_dataset_with_ph_temp)\n",
    "km_cluster_init = get_cluster(km_dataset_with_ph_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab3b9e",
   "metadata": {},
   "source": [
    "#### (c) Extract buffer information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ab8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [row[6] for _,rows in kcat_cluster_init.items() for row in rows]\n",
    "comments += [row[6] for _,rows in km_cluster_init.items() for row in rows]\n",
    "comments = list(set(comments))\n",
    "\n",
    "with open(\"./brenda_data_cache/comment.txt\",'w') as f:\n",
    "    for com in comments:\n",
    "        f.write(com+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb66495",
   "metadata": {},
   "source": [
    "We used LLM and manual review to extract buffer information from each comment in `comment.txt`, producing a dictionary mapping comments to buffer information. Then, we used this dictionary to further subdivide the existing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_split_cluster(cluster_dict, target):\n",
    "    buffer_dict = pickle.load(open(\"./brenda_data_cache/buffer_mapping.pkl\",'rb'))\n",
    "    for pair_name in cluster_dict:\n",
    "        rows = cluster_dict[pair_name]\n",
    "        for i in range(len(rows)):\n",
    "            print(len(rows[i]))\n",
    "            rows[i].append(buffer_dict[rows[i][6]])\n",
    "        cluster_dict[pair_name] = rows\n",
    "    \n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        buffers = [row[9] for row in rows]\n",
    "        # All data have the same buffer\n",
    "        if len(set(buffers))==1:\n",
    "            continue\n",
    "        # There are different buffers in a cluster\n",
    "        cluster_dict.pop(pair_name)\n",
    "        for buffer in set(buffers):\n",
    "            new_pair_name = pair_name+\";;;\"+str(buffer)\n",
    "            new_rows = [row for row in rows if row[9] == buffer]\n",
    "            type = [1 if 'wild' in row[6] else 0 for row in sub_grouped_buffer_rows]\n",
    "            if type.count(1)==0 or type.count(0)==0: # exclude clusters lacking wildtype or mutant\n",
    "                continue\n",
    "            cluster_dict[new_pair_name] = new_rows\n",
    "    return cluster_dict\n",
    "kcat_cluster = buffer_split_cluster(kcat_cluster_init,\"kcat\")\n",
    "km_cluster = buffer_split_cluster(km_cluster_init,'km')\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81fb6b",
   "metadata": {},
   "source": [
    "# 4. Enzyme information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00590408",
   "metadata": {},
   "source": [
    "#### (a) Download UniProt ID by EC number and species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a5c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.email = \"your email address\"\n",
    "def get_taxonomy_id(organism_name):\n",
    "    # Search the taxonomy ID by species name\n",
    "    max_attempts = 5\n",
    "    res = -1 \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            handle = Entrez.esearch(db=\"taxonomy\", term=organism_name)\n",
    "            record = Entrez.read(handle)\n",
    "            handle.close()\n",
    "\n",
    "            if record[\"IdList\"]:\n",
    "                tax_id = record[\"IdList\"][0]  \n",
    "                handle = Entrez.efetch(db=\"taxonomy\", id=tax_id, retmode=\"xml\")\n",
    "                if handle:\n",
    "                    records = Entrez.read(handle)\n",
    "                    lineage = records[0][\"Lineage\"]  \n",
    "                    res = lineage\n",
    "                handle.close()\n",
    "            break\n",
    "        except:\n",
    "            if attempt == max_attempts-1:\n",
    "                print(organism_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: EC number; 1: species\n",
    "ec_organism_pair = {\";;;\".join([rows[0][0],rows[0][1]]):-1  for rows in list(kcat_cluster.values())+list(km_cluster.values()) if rows[0][2] == '-'}\n",
    "\n",
    "# Construct the BRENDA client in step 1.\n",
    "for ec_org in tqdm(list(ec_organism_pair.keys())):\n",
    "    max_attempts = 5\n",
    "    for attempt in range(max_attempts): \n",
    "        try:\n",
    "            ec,org = ec_org.split(\";;;\")\n",
    "            parameters = (email,password,f\"ecNumber*{ec}\", \"sequence*\",\"noOfAminoAcids*\", \"firstAccessionCode*\",\"source*\",\"id*\",\n",
    "                        f\"organism*{org}\")\n",
    "            sequence = client.service.getSequence(*parameters)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(f\"Attempt {attempt + 1} times. Exception: {Exception}\")\n",
    "                sequence = -1  \n",
    "    if sequence==-1 or len(list(sequence))!=1:\n",
    "        ec_organism_pair.pop(ec_org)\n",
    "        continue\n",
    "    \n",
    "    # Check whether the species is bacteria\n",
    "    organism_name = ec_org.split(\";;;\")[1]  \n",
    "    lineage = get_taxonomy_id(organism_name)\n",
    "    if lineage and ('Bacteria' in lineage or 'bacteria' in lineage):\n",
    "        ec_organism_pair[ec_org]= list(sequence)[0]\n",
    "    else:\n",
    "        ec_organism_pair.pop(ec_org)\n",
    "len(ec_organism_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing UniProt IDs, and then discard those still without IDs\n",
    "def add_uniprotId(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        uniprot = rows[0][2]\n",
    "        ec_org = \";;;\".join([rows[0][0],rows[0][1]])\n",
    "        if uniprot != '-':\n",
    "            continue\n",
    "        if ec_org not in ec_organism_pair:\n",
    "            cluster_dict.pop(pair_name)\n",
    "            continue\n",
    "        new_uniprot = ec_organism_pair[ec_org][0]['firstAccessionCode']\n",
    "        for i in range(len(rows)):\n",
    "            rows[i][2]=new_uniprot\n",
    "        cluster_dict[pair_name]=rows\n",
    "    return cluster_dict\n",
    "\n",
    "kcat_cluster = add_uniprotId(kcat_cluster)\n",
    "km_cluster = add_uniprotId(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761bd90",
   "metadata": {},
   "source": [
    "#### (b) Download sequences and verify mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7da99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "UIDs = list(set([rows[0][2] for _,rows in kcat_cluster.items()] + [rows[0][2] for _,rows in km_cluster.items()]))\n",
    "UID_Seq_dict = dict()\n",
    "for id in tqdm(UIDs):\n",
    "    url = \"https://www.uniprot.org/uniprot/%s.fasta\" % id\n",
    "    try :\n",
    "        data = request.urlopen(url)\n",
    "        respdata = data.read().decode(\"utf-8\").strip()\n",
    "        seq = ''.join([i for i in respdata.split('\\n')[1:]])\n",
    "        UID_Seq_dict[id] =  seq\n",
    "    except :\n",
    "        print(id, \"can not find from uniprot!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_aa = ['U','O','X','B','J','Z']\n",
    "def check_aa(seq):\n",
    "    for aa in seq:\n",
    "        if aa in error_aa:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_mutant(cluster_dict):\n",
    "    for pair_name in list(cluster_dict.keys()):\n",
    "        rows = cluster_dict[pair_name]\n",
    "        uniprotId = rows[0][2]\n",
    "        seq = UID_Seq_dict[uniprotId]\n",
    "        if not check_aa(seq):\n",
    "            cluster_dict.pop(pair_name)\n",
    "            \n",
    "        \n",
    "        wt_rows = [row for row in rows if len(extract_mutations(row[6]))==0]\n",
    "        mut_rows = [row for row in rows if len(extract_mutations(row[6]))!=0]\n",
    "        mut_rows_new = []\n",
    "        for row in mut_rows:\n",
    "            flag=True\n",
    "            mut_loc = extract_mutations(row[6])\n",
    "            for mut in mut_loc:\n",
    "                loc = int(mut[1:-1])\n",
    "                if loc >= len(seq)-1 or mut[0] not in [seq[loc-1],seq[loc],seq[loc+1]]: \n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                # If it is a multiple-point mutation, the indices of all mutated residues should based on the same starting position (-1, 0 or 1)\n",
    "                all_flag=False\n",
    "                for dev in range(-1,2,1):\n",
    "                    flag=True\n",
    "                    for mut in mut_loc:\n",
    "                        loc = int(mut[1:-1])+dev\n",
    "                        if mut[0] != seq[loc]:\n",
    "                            flag=False\n",
    "                            break\n",
    "                    if flag:\n",
    "                        all_flag=True\n",
    "                        break\n",
    "                if all_flag: \n",
    "                    mut_rows_new.append(row)\n",
    "        if len(mut_rows_new)==0: \n",
    "            cluster_dict.pop(pair_name)\n",
    "        else:\n",
    "            cluster_dict[pair_name] = wt_rows + mut_rows_new\n",
    "    return cluster_dict\n",
    "kcat_cluster = check_mutant(kcat_cluster)\n",
    "km_cluster = check_mutant(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59b3e7",
   "metadata": {},
   "source": [
    "#### (c) Deduplication\n",
    "Deduplication of multiple wildtypes or multiple identical mutant data within a cluster requires manual intervention to ensure accuracy. Here, we exported the comments in the clusters to a text file for manual processing when controversies existed. For example, some clusters may contain modified enzymes (indicated by suffixes), which should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71e4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(cluster_dict):\n",
    "    for pair_name in cluster_dict:\n",
    "        wt_rows = [row for row in cluster_dict[pair_name] if 'wild' in row[6]]\n",
    "        mut_rows = [row for row in cluster_dict[pair_name] if len(extract_mutations(row[6]))>0]\n",
    "        # wt\n",
    "        if len(wt_rows)!=1:\n",
    "            K = \",\".join([row[5] for row in wt_rows])\n",
    "            wt_rows[0][5] = K\n",
    "        wt_row = wt_rows[0]\n",
    "        # mut\n",
    "        mut_locs = [\",\".join(extract_mutations(row[6])) for row in mut_rows]\n",
    "        if len(set(mut_locs))!=len(mut_locs):\n",
    "            # Put the same mutation points in the same list \n",
    "            grouped_mut_rows = [[row for row in mut_rows if \",\".join(extract_mutations(row[6]))==mut_loc] for mut_loc in set(mut_locs)]\n",
    "            # Then put the kcat of all records in each sublist together and throw it to the first record in the sublist\n",
    "            mut_rows = []\n",
    "            for sub_grouped_mut_rows in grouped_mut_rows:\n",
    "                K = \",\".join([row[5] for row in sub_grouped_mut_rows])\n",
    "                sub_grouped_mut_rows[0][5] = K # The kcat of the first record stores all kcats of the same mutations\n",
    "                mut_rows.append(sub_grouped_mut_rows[0])\n",
    "        cluster_dict[pair_name] = [wt_row] + mut_rows\n",
    "    return cluster_dict\n",
    "    \n",
    "# The cluster here is the cluster that has been manually confirmed\n",
    "kcat_cluster = remove_duplicate(kcat_cluster)\n",
    "km_cluster = remove_duplicate(km_cluster)\n",
    "len(kcat_cluster),len(km_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a65af1",
   "metadata": {},
   "source": [
    "# 5. Construct mutation effect pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revised_mut_loc(mut_loc,seq):    \n",
    "    # If it is a multiple-point mutation, the indices of all mutated residues should based on the same starting position (-1, 0 or 1)\n",
    "    all_flag=False\n",
    "    true_dev = False\n",
    "    for dev in range(-1,2,1):\n",
    "        flag=True\n",
    "        for mut in mut_loc:\n",
    "            loc = int(mut[1:-1])+dev\n",
    "            if mut[0] != seq[loc]:\n",
    "                flag=False\n",
    "                break\n",
    "        if flag:\n",
    "            all_flag=True\n",
    "            true_dev=dev\n",
    "            break\n",
    "    if all_flag:\n",
    "        mut_loc_new = [mut[0]+str(int(mut[1:-1])+true_dev)+mut[-1] for mut in mut_loc]\n",
    "        return mut_loc_new\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5947cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cluster_dict,target):\n",
    "    EcNumber = []\n",
    "    organism = []\n",
    "    substrate = []\n",
    "    UniprotId = []\n",
    "    pubmedId = []\n",
    "    temperature = []\n",
    "    pH = []\n",
    "    buffer = []\n",
    "    sequence = []\n",
    "    mutant = [] \n",
    "    wt_ks = []\n",
    "    mut_ks = []\n",
    "    delta_ks = []\n",
    "\n",
    "    for pair_name in cluster_dict:\n",
    "        wt_row = [row for row in cluster_dict[pair_name] if 'wild' in row[6]][0]\n",
    "        UID,wt_K = wt_row[2],wt_row[5]\n",
    "        wt_K = [float(i) for i in wt_row[5].split(',')]\n",
    "        wt_K = np.mean([math.log10(i) for i in wt_K])\n",
    "        seq = IdSeq_dict[UID]\n",
    "        \n",
    "        mut_rows = [row for row in cluster_dict[pair_name] if len(extract_mutations(row[6]))>0]\n",
    "        for mut_row in mut_rows:\n",
    "            mutant_info = revised_mut_loc(extract_mutations(mut_row[6]),seq)\n",
    "            if mutant_info==-1: # confusing sequence starting index\n",
    "                print(extract_mutations(mut_row[6]),seq)\n",
    "                continue\n",
    "            \n",
    "            mut_K = mut_row[5]\n",
    "            mut_K = [float(i) for i in mut_row[5].split(\",\")]\n",
    "            mut_K = np.mean([math.log10(i) for i in mut_K])\n",
    "\n",
    "            EcNumber.append(wt_row[0])\n",
    "            organism.append(wt_row[1].lower())\n",
    "            substrate.append(wt_row[4].lower())\n",
    "            UniprotId.append(UID)\n",
    "            pubmedId.append(wt_row[3])\n",
    "            temperature.append(wt_row[7])\n",
    "            pH.append(wt_row[8])\n",
    "            buffer.append(wt_row[9])\n",
    "            sequence.append(seq)\n",
    "            mutant.append(\",\".join(mutant_info))\n",
    "            wt_ks.append(wt_K)\n",
    "            mut_ks.append(mut_K)\n",
    "            delta_ks.append(mut_K-wt_K)\n",
    "    df = pd.DataFrame({\n",
    "        'EcNumber':EcNumber,'Organism':organism,\"Substrate\":substrate,\n",
    "        'UniprotId':UniprotId,'brenda_Ref_Id':pubmedId,\n",
    "        'Temperature':temperature,'pH':pH,'buffer':buffer,\n",
    "        'sequence':sequence,'mutant':mutant,\n",
    "        f'wt_{target}_log10':wt_ks,f'mut_{target}_log10':mut_ks,f'delta_{target}_log10':delta_ks,\n",
    "    })\n",
    "    return df\n",
    "kcat_df = create_df(kcat_cluster,'kcat')\n",
    "km_df = create_df(km_cluster,'km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "kcat_df_avg = kcat_df.groupby(['EcNumber', 'Organism','Substrate','UniprotId',\n",
    "                               'brenda_Ref_Id','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_kcat_log10','mut_kcat_log10'], as_index=False).agg({'delta_kcat_log10': 'mean'})\n",
    "km_df_avg = km_df.groupby(['EcNumber', 'Organism','Substrate','UniprotId',\n",
    "                               'brenda_Ref_Id','Temperature','pH','buffer','sequence','mutant',\n",
    "                               'wt_km_log10','mut_km_log10'], as_index=False).agg({'delta_km_log10': 'mean'})\n",
    "\n",
    "kcat_df_avg.to_csv(\"./brenda_data_cache/brenda_delta_kcat_df.csv\",index=False)\n",
    "km_df_avg.to_csv(\"./brenda_data_cache/brenda_delta_km_df.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GVP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
